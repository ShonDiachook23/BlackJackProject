import numpy as np
import random
from collections import deque
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
from BlackJackEnv import BlackjackEnv, Action  # ×™×™×‘×•× ×¡×‘×™×‘×ª ×”××©×—×§ ×•×”×¤×¢×•×œ×•×ª ×”××¤×©×¨×™×•×ª

# ×¡×•×›×Ÿ ××‘×•×¡×¡ Deep Q-Network
class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size  # ×’×•×“×œ ×”×•×•×§×˜×•×¨ ×©××ª××¨ ××ª ××¦×‘ ×”×¡×‘×™×‘×” (state)
        self.action_size = action_size  # ××¡×¤×¨ ×”×¤×¢×•×œ×•×ª ×”××¤×©×¨×™×•×ª (Hit, Stand)

        # ×–×™×›×¨×•×Ÿ ×œ×©××™×¨×ª ×—×•×•×™×•×ª ×§×•×“××•×ª â€“ ×œ×¦×•×¨×š Replay
        self.memory = deque(maxlen=10000)

        # ×”×™×¤×¨-×¤×¨××˜×¨×™× ×œ×œ××™×“×”
        self.gamma = 0.95             # ×¤×§×˜×•×¨ ×”× ×—×” â€“ ×§×•×‘×¢ ×¢×“ ×›××” ××ª×—×©×‘×™× ×‘×ª×’××•×œ×™× ×¢×ª×™×“×™×™×
        self.epsilon = 1.0            # ×”×¡×ª×‘×¨×•×ª ×œ×‘×—×™×¨×” ××§×¨××™×ª ×©×œ ×¤×¢×•×œ×” (exploration)
        self.epsilon_min = 0.01       # ×¢×¨×š ××™× ×™××œ×™ ×œ-epsilon
        self.epsilon_decay = 0.995    # ×§×¦×‘ ×“×¢×™×›×” ×©×œ epsilon ×‘×›×œ ××¤×™×–×•×“×”

        self.model = self.build_model()  # ×‘× ×™×™×ª ×”×¨×©×ª ×”× ×•×™×¨×•× ×™×ª

    # ×‘× ×™×™×ª ×¨×©×ª × ×•×™×¨×•× ×™× ×¤×©×•×˜×” ×¢× ×©×ª×™ ×©×›×‘×•×ª ×—×‘×•×™×•×ª
    def build_model(self):
        model = Sequential()
        model.add(Dense(24, input_dim=self.state_size, activation='relu'))  # ×©×›×‘×ª ×§×œ×˜ + ReLU
        model.add(Dense(24, activation='relu'))                             # ×©×›×‘×ª ×—×‘×•×™×” × ×•×¡×¤×ª
        model.add(Dense(self.action_size, activation='linear'))            # ×©×›×‘×ª ×¤×œ×˜ â€“ ×¢×¨×š Q ×œ×›×œ ×¤×¢×•×œ×”
        model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))     # ××™××•×Ÿ ×¢× ×¤×•× ×§×¦×™×™×ª ×”×¤×¡×“ MSE
        return model

    # ×©××™×¨×ª ×—×•×•×™×” ×‘×–×™×›×¨×•×Ÿ: ××¦×‘, ×¤×¢×•×œ×”, ×ª×’××•×œ, ××¦×‘ ×”×‘×, ×”×× ×”×¡×ª×™×™×
    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    # ×‘×—×™×¨×ª ×¤×¢×•×œ×” â€“ ××• ×œ×¤×™ ×”×¨×©×ª (exploit) ××• ×‘××§×¨××™ (explore)
    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return random.choice(list(Action))  # ×¤×¢×•×œ×” ××§×¨××™×ª (×—×§×¨)
        act_values = self.model.predict(np.array([state]), verbose=0)
        return Action(np.argmax(act_values[0]))  # ×¤×¢×•×œ×” ×¢× ×¢×¨×š Q ×”×’×‘×•×” ×‘×™×•×ª×¨

    # ×œ××™×“×” ××”×–×™×›×¨×•×Ÿ: replay ×©×œ ×“×•×’×××•×ª ×§×•×“××•×ª
    def replay(self, batch_size=32):
        # ×“×’×™××” ××§×¨××™×ª ×©×œ ××™× ×™Ö¾×‘××¥'
        minibatch = random.sample(self.memory, min(len(self.memory), batch_size))

        for state, action, reward, next_state, done in minibatch:
            target = reward
            if not done:
                # ×× ×”××©×—×§ ×œ× ×”×¡×ª×™×™× â€“ ×”×•×¡×¤×ª ×¢×¨×š Q ×¢×ª×™×“×™ (×œ×¤×™ ××§×¡×™××•× ×¤×¢×•×œ×•×ª ×¢×ª×™×“×™×•×ª)
                target += self.gamma * np.amax(self.model.predict(np.array([next_state]), verbose=0)[0])

            # ×—×™×–×•×™ ×¢×¨×›×™ Q × ×•×›×—×™×™× ×•×¢×“×›×•×Ÿ ×”×¤×¢×•×œ×” ×©×‘×•×¦×¢×” ×œ×¤×™ ×”-target
            target_f = self.model.predict(np.array([state]), verbose=0)
            target_f[0][action.value] = target

            # ×”×ª×××ª ×”×¨×©×ª ×œ×“×•×’××” ×”×—×“×©×”
            self.model.fit(np.array([state]), target_f, epochs=1, verbose=0)

        # ×¢×“×›×•×Ÿ epsilon â€“ ×¦××¦×•× ×¨××ª ×”×—×§×¨ ×¢× ×”×–××Ÿ
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

# ×‘×œ×•×§ ×”×¤×¢×œ×” â€“ ××™××•×Ÿ ×”×¡×•×›×Ÿ
if __name__ == '__main__':
    env = BlackjackEnv()  # ×™×¦×™×¨×ª ×¡×‘×™×‘×”
    agent = DQNAgent(state_size=3, action_size=len(Action))  # ×™×¦×™×¨×ª ×¡×•×›×Ÿ

    episodes = 1000  # ××¡×¤×¨ ××¤×™×–×•×“×•×ª ××™××•×Ÿ
    total_balance = 0

    for e in range(episodes):
        state = env.reset()
        done = False
        episode_reward = 0

        while not done:
            action = agent.act(state)
            next_state, reward, done = env.step(action)
            agent.remember(state, action, reward, next_state, done)
            state = next_state
            episode_reward += reward
            total_balance += reward

        agent.replay()

        # ×”×“×¤×¡×” ×‘×›×œ ××¤×™×–×•×“×”
        print(f"[{e+1}/{episodes}] Epsilon: {agent.epsilon:.4f} | Episode Reward: {episode_reward:.2f} | Total Balance: {total_balance:.2f}")

        # ×©××™×¨×” ×›×œ 500 ××¤×™×–×•×“×•×ª
        if (e + 1) % 500 == 0:
            agent.model.save("blackjack_model.keras")
            print("ğŸŸ¢ Intermediate model saved (blackjack_model.keras)")

# ×©××™×¨×” ×¡×•×¤×™×ª
agent.model.save("blackjack_model.keras")
print("âœ… Final model saved: blackjack_model.keras")

agent.model.save("dqn_blackjack_model.h5")
print("âœ… Final model saved: dqn_blackjack_model.h5")
